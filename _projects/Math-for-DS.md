---
title: 'Math for DS is Coming!'
subtitle: 'Online Seminar on  Mathematical Foundations of Data Science'
date: 2020-05-18 12:00:00
description: A weekly online seminar on random topics on mathematical foundations of machine learning, statistics and optimization
featured_image: '/images/news/MathDS/cover.jpg'
---

## Original official website:
Click [here](https://sites.google.com/view/seminarmathdatascience/home)

## Mailing List and Calendar

For announcements and Zoom links, please [subscribe](https://docs.google.com/forms/d/e/1FAIpQLSfFidZVxlQKpaSc7Deu80gKoflvgYSQspST0l1UyhD6vkZfIA/viewform?usp=sf_link) to our mailing list.

You may add the talks to your calendar by clicking [here](https://www.google.com/calendar/render?cid=princeton.edu_rn1k9ev6hgesqaskquv54mb71g@group.calendar.google.com).

## To ask the speaker a question...

To ask the speaker a question, please use the Q&A function in Zoom to type your questions. The moderator (Zhuoran Yang) will collect questions through the talk and ask the questions after the talk.

## Next speaker

 <p align="center"><img width="20%" src="/images/news/MathDS/kpotufe.jpg" /></p>

##### **Next Speaker:** [**Samory Kpotufe**](http://www.columbia.edu/~skk2175/), Columbia University
##### **Date/Time:** Tuesday, July 21st, 3pm EDT [**[Zoom Link]**](https://psu.zoom.us/j/95512102924) [**[Facebook Link]**](https://www.facebook.com/events/2975077669258072)
##### **Title:** Some Recent Insights on Transfer-Learning

**Abstract:** A common situation in Machine Learning is one where training data is not fully representative of a target population due to bias in the sampling mechanism or high costs in sampling the target population; in such situations, we aim to ’transfer’ relevant information from the training data (a.k.a. source data) to the target application. How much information is in the source data? How much target data should we collect if any? These are all practical questions that depend crucially on 'how far' the source domain is from the target. However, how to properly measure 'distance' between source and target domains remains largely unclear.

In this talk we will argue that much of the traditional notions of 'distance' (e.g. KL-divergence, extensions of TV such as D_A discrepancy, density-ratios, Wasserstein distance) can yield an over-pessimistic picture of transferability. Instead, we show that some new notions of 'relative dimension' between source and target (which we simply term 'transfer-exponents') capture a continuum from easy to hard transfer. Transfer-exponents uncover a rich set of situations where transfer is possible even at fast rates, encode relative benefits of source and target samples, and have interesting implications for related problems such as multi-task or multi-source learning.

In particular, in the case of multi-source learning, we will discuss (if time permits) a strong dichotomy between minimax and adaptive rates: no adaptive procedure can achieve a rate better than single source rates, although minimax (oracle) procedures can.

The talk is based on earlier work with Guillaume Martinet, and ongoing work with Steve Hanneke.


**Bio:** Professor Samory Kpotufe graduated in 2010 from Computer Science at the University of California, San Diego and was advised by Sanjoy Dasgupta. He then was a researcher at the Max Planck Institute for Intelligent Systems working in the department of Bernhard Schoelkopf in the learning theory group of Ulrike von Luxburg. Following this, he was an Assistant Research Professor at the Toyota Technological Institute at Chicago. He then spent over 4 years at ORFE, Princeton University as Assistant Professor.

Professor Kpotufe's work is in machine learning, with an emphasis on nonparametric methods and high-dimensional statistics. Generally, he is interested in understanding the inherent difficulty of high-dimensional problems, under practical constraints from real-world application domains. The nonparametric setting is attractive in that it captures scenarios where we have little domain knowledge, which is important as data sciences reach into a diverse range of applications.

His main practical aim is to design adaptive procedures, i.e., practical procedures that can self-tune to unknown structure in data (e.g., manifold, sparsity, clusters), while at the same time meeting the various constraints (e.g., time, space, labeling cost) of modern applications.

## Upcoming Speakers

![](/images/news/MathDS/speakers.png)

## Organizers

If you have any questions, please feel free to email the organizers.

[Ethan X. Fang](http://www.personal.psu.edu/xxf13/), [Niao He](http://niaohe.ise.illinois.edu/), [Junwei Lu](https://www.hsph.harvard.edu/junwei-lu/), [Zhaoran Wang](https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/wang-zhaoran.html),  [Zhuoran Yang](http://www.princeton.edu/~zy6/), [Tuo Zhao](https://www2.isye.gatech.edu/~tzhao80/)

## Sponsors

[![Alt text](/images/news/MathDS/GaTech.png) Georgia Institute of Technology](https://www.gatech.edu/)

[![Alt text](/images/news/MathDS/Harvard.png) Harvard University](https://www.harvard.edu/)

[![Alt text](/images/news/MathDS/NWU.png) Northwestern University](https://www.northwestern.edu/)

[![Alt text](/images/news/MathDS/PSU.png) Pennsylvania State University](https://www.psu.edu/)

[![Alt text](/images/news/MathDS/Princeton.png) Princeton University](https://www.princeton.edu/)

[![Alt text](/images/news/MathDS/UIUC.png) University of Illinois at Urbana-Champaign](https://illinois.edu/)

[![Alt text](/images/news/MathDS/NISS.png) National Institute of Statistical Sciences](https://www.niss.org/)

[![Alt text](/images/news/MathDS/2sigma.png) Two Sigma](https://www.twosigma.com/)

[![Alt text](/images/news/MathDS/ORAI.png) ORAI China](/)
